{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to transform the json format columns in table\n",
    "def load_df(csv_path='./Data/train.csv', n_rows=None):\n",
    "    json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    # Importing the dataset\n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in json_cols}, # Loading the json columns properly\n",
    "                     dtype={'fullVisitorId': 'str'}, # Transforming this column to string\n",
    "                     nrows=n_rows)# Number of rows that will be imported randomly\n",
    "    \n",
    "    for column in json_cols: # Loop to finally transform the columns in data frame\n",
    "        # This will normalize and set the json to a table\n",
    "        column_as_df = json_normalize(df[column]) \n",
    "        # The column name is set using the category and subcategory of json columns\n",
    "        column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns] \n",
    "        # After extracting the values, we drop the original columns\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "        \n",
    "    # Printing the shape of dataframes that was imported     \n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df #Returning the df after importing and transforming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the data frame\n",
    "df_train = load_df() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_train.head() # First five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find columns with a single value (non-informative columns) and drop them\n",
    "def uniq_col_dropper(df):\n",
    "    cols_uniq_value = [col for col in df.columns if df[col].nunique(dropna=False) == 1]\n",
    "    df.drop(cols_uniq_value, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with a unique value from the data\n",
    "df_train = uniq_col_dropper(df_train) \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_train.head() # First five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the number of missing values\n",
    "def missing_values(df):\n",
    "    count_null = df.isnull().sum().sort_values(ascending=False) # Counting missing values and sorting\n",
    "    percent_null = count_null / len(df) * 100  #  Percentage of rows with a missing value in each column\n",
    "    null = pd.concat([count_null, percent_null], axis=1, keys=['count_null', 'percent_null']) # Concatenating count and percent\n",
    "    \n",
    "    #percent_nonnull_tranrev = 100 - percent_null['totals.transactionRevenue']\n",
    "    \n",
    "    print(\"Columns with at least one value: \")\n",
    "    print(null[(null['count_null']!=0)]) # Returning info of columns with at least one value\n",
    "    #print(f\"\\n Percentage of sessions with transactions data: {percent_nonnull_tranrev:.3f}\") # Percentage of sessions with transactions\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values(df_train) # Info on missing value in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with missing data and changing column types\n",
    "def fill_na(df):    # fillna numeric feature\n",
    "    df['totals_pageviews'].fillna(1, inplace=True) #filling NA's with 1\n",
    "    df['totals_newVisits'].fillna(0, inplace=True) #filling NA's with 0\n",
    "    df['totals_bounces'].fillna(0, inplace=True)   #filling NA's with 0\n",
    "    df['trafficSource_isTrueDirect'].fillna(False, inplace=True) # filling boolean with False\n",
    "    df['trafficSource_adwordsClickInfo.isVideoAd'].fillna(True, inplace=True) # filling boolean with True\n",
    "    df[\"totals_transactionRevenue\"] = df[\"totals_transactionRevenue\"].fillna(0.).astype(float) #filling NA with 0\n",
    "    df['totals_pageviews'] = df['totals_pageviews'].astype(int) # setting numerical column as integer\n",
    "    df['totals_newVisits'] = df['totals_newVisits'].astype(int) # setting numerical column as integer\n",
    "    df['totals_bounces'] = df['totals_bounces'].astype(int)  # setting numerical column as integer\n",
    "    df[\"totals_hits\"] = df[\"totals_hits\"].astype(int) # setting numerical to float\n",
    "    df['totals_newVisits'] = df['totals_newVisits'].astype(int) # seting as int\n",
    "    return df # Return the transformed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with missing data\n",
    "df_train = fill_na(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find columns with a single value (non-informative columns) and drop them\n",
    "def uniq_col_drop_final(df):\n",
    "    cols_uniq_value = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    df.drop(cols_uniq_value, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with a unique value from the data\n",
    "df_train = uniq_col_drop_final(df_train) \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_train.head(5) # First three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that are directly derived from other columns\n",
    "df_train = df_train.drop(['totals_bounces', 'totals_newVisits'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the duplicated revenue and hits by their sum and other features with the first entry when sessionId is the same\n",
    "def dup_remover(df):\n",
    "    g_cols = ['sessionId'] # Group by this column\n",
    "    sum_cols = ['totals_hits', 'totals_pageviews', 'totals_transactionRevenue'] # Return sums of these columns\n",
    "    min_cols = ['visitStartTime'] # Return the minimum of this column\n",
    "    cols = df_train.columns[~df_train.columns.isin(sum_cols+min_cols+g_cols)]\n",
    "    d_sum = {col:'sum' for col in sum_cols} # dict comprehension for the sum of columns\n",
    "    d_min = {col:'min' for col in min_cols} # dict comprehension for the min of columns\n",
    "    d = {col:'first' for col in cols} # dict comprehension for the first column\n",
    "    d.update(d_sum)\n",
    "    d.update(d_min)\n",
    "    df = df.groupby(g_cols).agg(d).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dup_remover(df_train) # Remove duplicates\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "dup = df_train[df_train.duplicated(['sessionId'], keep=False)].sort_values(by=['sessionId','date'])\n",
    "\n",
    "dup[dup.sessionId.isin(dup[dup['totals_transactionRevenue'].notnull()].sessionId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with the time data\n",
    "def date_process(df):\n",
    "    df['date'] = pd.to_datetime(df.date, format='%Y%m%d') # Convert to datetime format\n",
    "    # Extract year, month, day, and day of week from date\n",
    "    df['_year'] = df.date.dt.year\n",
    "    df['_month'] = df.date.dt.month\n",
    "    df['_day'] = df.date.dt.day\n",
    "    df['_dayofWeek'] = df.date.dt.dayofweek\n",
    "    # Extract hour from the \"visitStartTime\" column\n",
    "    df['_hour'] = pd.to_datetime(df.visitStartTime, unit='s').dt.hour\n",
    "    df.drop(['date', 'visitStartTime'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = date_process(df_train) # Getting time and date data\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the variables\n",
    "def normalize(df):\n",
    "    # Use MinMaxScaler to normalize the column\n",
    "    #df['totals.hits'] =  (df['totals.hits'] - min(df['totals.hits'])) / (max(df['totals.hits'])  - min(df['totals.hits']))\n",
    "    # Normalizing the transaction Revenue\n",
    "    df['totals_transactionRevenue'] = df_train['totals_transactionRevenue'].apply(lambda x: np.log1p(x))\n",
    "    # return the modified df\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = normalize(df_train) # Normalizing two parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Save the flattened data set\n",
    "df_train.to_csv(\"./Data/train-flattened.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "# Read the flattened data set to check it and also the time\n",
    "train_flattened = pd.read_csv('./Data/train-flattened.csv', dtype={'fullVisitorId': 'str'}, index_col='sessionId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "train_flattened.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
